{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from PyPDF2 import PdfFileMerger\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "def get_html_source(url, sleep_time=10):\n",
    "    browser = webdriver.Chrome()\n",
    "    # get web page\n",
    "    browser.get(url)\n",
    "    # execute script to scroll down the page\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    # sleep for time\n",
    "    time.sleep(sleep_time)\n",
    "    return browser.page_source\n",
    "\n",
    "# returns list of links of all pdf on a page\n",
    "def get_all_hrefs(list_of_page_number_on_website, sleep_time=10):\n",
    "    browser = webdriver.Chrome()\n",
    "    final_all_pdf_links = []\n",
    "    for number in list_of_page_number_on_website:\n",
    "        # get web page\n",
    "        browser.get(f\"https://registry.verra.org/app/projectDetail/VCS/{number}\")\n",
    "        # execute script to scroll down the page( copy from internet )\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        # sleep for time\n",
    "        time.sleep(sleep_time)\n",
    "        page_source = browser.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        a_tags = soup.find_all(name=\"a\")\n",
    "        all_pdf_links_of_page = []\n",
    "        for a_tag in a_tags:\n",
    "            if a_tag.text[-3:] == \"pdf\":\n",
    "                all_pdf_links_of_page.append(a_tag[\"href\"])\n",
    "        del a_tags\n",
    "        final_all_pdf_links.append(all_pdf_links_of_page)\n",
    "    \n",
    "    return final_all_pdf_links\n",
    "\n",
    "# MAIN SCRIPT IMPLEMENTATION\n",
    "start_page= int(2)\n",
    "end_page= int(6)\n",
    "mylist = list(range(start_page, end_page+1))\n",
    "mode = \"r\" # read or write mode\n",
    "\n",
    "try:\n",
    "    os.mkdir(\"pdf_downloader_folder\")   \n",
    "except FileExistsError:\n",
    "    pass\n",
    "  \n",
    "if mode == \"w\" :\n",
    "    # ----- Writing to csv file\n",
    "    linksss = get_all_hrefs(mylist,8)\n",
    "\n",
    "    # data to be written row-wise in csv fil\n",
    "    data = linksss\n",
    "    # opening the csv file in 'w+' mode\n",
    "    file = open('pdf_downloader_folder/links.csv', 'w+', newline ='')\n",
    "\n",
    "    # writing the data into the file\n",
    "    with file:\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(data)\n",
    "    file.close()\n",
    "\n",
    "elif mode == \"r\":\n",
    "    # ------- Reading From csv file\n",
    "    with open('pdf_downloader_folder/links.csv', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    f.close()\n",
    "    linksss = data\n",
    "# --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and merge pdfs\n",
    "def create_and_merge_pdfs(links_list, start_page, end_page):\n",
    "    pdf_name_list = []\n",
    "    initial_start_page = start_page\n",
    "    merger = PdfFileMerger()\n",
    "    for a_page in links_list:\n",
    "        for index_link, a_link in enumerate(a_page):\n",
    "            content = requests.get(a_link).content\n",
    "            if content is None:\n",
    "                print (\"None success executed\")\n",
    "                continue\n",
    "            with open(f\"pdf_downloader_folder/{start_page}_{index_link}.pdf\", 'wb') as my_data:\n",
    "                my_data.write(content)\n",
    "            pdf_name_list.append(f\"pdf_downloader_folder/{start_page}_{index_link}.pdf\")\n",
    "            merger.append(f\"pdf_downloader_folder/{start_page}_{index_link}.pdf\")\n",
    "            \n",
    "        time.sleep(random.random()*3)\n",
    "        # Forcing correct name of pdf according to the start_page and end page \n",
    "        start_page += 1\n",
    "\n",
    "    merger.write(f\"pdf_downloader_folder/Compiled_{initial_start_page}_{end_page}.pdf\")\n",
    "    merger.close()\n",
    "    for pdf in pdf_name_list:\n",
    "        os.remove(pdf)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_merge_pdfs(links_list=linksss, start_page=start_page, end_page=end_page)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daaa31c8034f1feb794b255046b334f8762dacc75c33e103641e2c034660438a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
