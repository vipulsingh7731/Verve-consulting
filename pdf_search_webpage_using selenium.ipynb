{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# to use webdriver you need chromedriver.exe in the same folder as the .py script\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from pikepdf import Pdf as PDF\n",
    "# returns list of links of all pdf on a page\n",
    "def get_all_hrefs_of_pdfs(url,browser_instance, sleep_time=10):\n",
    "    browser = browser_instance\n",
    "    \n",
    "    # get web page\n",
    "    browser.get(url)\n",
    "    # execute script to scroll down the page( copy from internet )\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    # sleep for time\n",
    "    \n",
    "    time.sleep(sleep_time)\n",
    "    page_source = browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    a_tags = soup.find_all(name=\"a\")\n",
    "    all_pdf_links_of_page = []\n",
    "    for a_tag in a_tags:\n",
    "        try:\n",
    "            if a_tag[\"href\"][-3:] == \"pdf\":\n",
    "                if a_tag[\"href\"][0] == \"/\":\n",
    "                    url_split = url.split(\"/\")\n",
    "                    mylink = \"/\".join(url_split[0:3]) + a_tag[\"href\"]\n",
    "                item_dict = {\n",
    "                    'title':a_tag.text,\n",
    "                    'link':mylink,\n",
    "                }\n",
    "                all_pdf_links_of_page.append(item_dict)\n",
    "        except :\n",
    "            pass\n",
    "    del a_tags\n",
    "    \n",
    "    return all_pdf_links_of_page\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
