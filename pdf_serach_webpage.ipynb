{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# to use webdriver you need chromedriver.exe in the same folder as the .py script\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from pikepdf import Pdf as PDF\n",
    "\n",
    "# returns list of links of all pdf on a page\n",
    "def get_all_hrefs_of_pdfs(url,browser_instance, sleep_time=3):\n",
    "    browser = browser_instance\n",
    "    \n",
    "    # get web page\n",
    "    # print(\"before\")\n",
    "    browser.get(url)\n",
    "    # execute script to scroll down the page( copy from internet )\n",
    "    try:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        # time.sleep(1)\n",
    "        browser.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    except :\n",
    "        pass\n",
    "    # sleep for time\n",
    "    # print(\"after\")\n",
    "    # time.sleep(sleep_time)\n",
    "    page_source = browser.page_source\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    a_tags = soup.find_all(name=\"a\")\n",
    "    all_pdf_links_of_page = []\n",
    "    if url[-4:]==\".pdf\":\n",
    "        item_dict = {\n",
    "            'title':browser.title,\n",
    "            'link':url,\n",
    "        }\n",
    "        print(item_dict['title'])\n",
    "        all_pdf_links_of_page.append(item_dict)\n",
    "    for a_tag in a_tags:\n",
    "        try:\n",
    "            if a_tag[\"href\"][-3:] == \"pdf\":\n",
    "                if a_tag[\"href\"][0] == \"/\":\n",
    "                    url_split = url.split(\"/\")\n",
    "                    mylink = \"/\".join(url_split[0:3]) + a_tag[\"href\"]\n",
    "                item_dict = {\n",
    "                    'title':a_tag.text,\n",
    "                    'link':mylink,\n",
    "                }\n",
    "                all_pdf_links_of_page.append(item_dict)\n",
    "        except :\n",
    "            pass\n",
    "    del a_tags\n",
    "    \n",
    "    return all_pdf_links_of_page\n",
    "\n",
    "# Create and merge pdfs\n",
    "# def create_and_merge_pdfs(links_list, start_page, end_page):\n",
    "#     pdf_name_list_which_were_created = []\n",
    "#     initial_start_page = start_page\n",
    "#     for a_page in links_list:\n",
    "#         pdf_ = PDF.new()\n",
    "#         version = pdf_.pdf_version\n",
    "#         if len(links_list[start_page-initial_start_page]) == 0:\n",
    "#             print(f\"---------Index {start_page} has no files\")\n",
    "#             pdf_.close()\n",
    "#             start_page += 1\n",
    "#             continue\n",
    "#         for index_link, a_link in enumerate(a_page):\n",
    "#             content = requests.get(a_link).content\n",
    "#             with open(f\"pdf_downloader_folder/{start_page}_{index_link}.pdf\", 'wb') as my_data:\n",
    "#                 my_data.write(content)\n",
    "#                 my_data.close()\n",
    "#             pdf_name_list_which_were_created.append(f\"pdf_downloader_folder/{start_page}_{index_link}.pdf\")\n",
    "            \n",
    "#             try:\n",
    "                \n",
    "#                 src = PDF.open(f\"pdf_downloader_folder/{start_page}_{index_link}.pdf\")\n",
    "#                 version = max(version, src.pdf_version)\n",
    "#                 pdf_.pages.extend(src.pages)\n",
    "#                 src.close()\n",
    "#                 print(f\"Fetched & Merged PageNumber_PDFindex-- {start_page}_{index_link}\")\n",
    "#             except:\n",
    "#                 print(f\"------------{start_page}_{index_link}.pdf was not merged due to some reason!!!\")\n",
    "#                 print(\"----It is written to a separate file\")\n",
    "#                 with open(f\"pdf_downloader_folder/Compiled_{start_page}_{index_link}_was.pdf\", 'wb') as f:\n",
    "#                     f.write(content)\n",
    "#                     f.close()\n",
    "#             time.sleep(random.random()*3)\n",
    "#         pdf_.remove_unreferenced_resources()\n",
    "#         pdf_.save(f\"pdf_downloader_folder/Compiled_{start_page}_.pdf\", min_version=version)\n",
    "#         pdf_.close()\n",
    "#         # merger.write(f\"pdf_downloader_folder/Compiled_{start_page}_.pdf\")\n",
    "#         # merger.close()\n",
    "#         start_page += 1\n",
    "\n",
    "    \n",
    "#     for pdf in pdf_name_list_which_were_created:\n",
    "#         os.remove(pdf)\n",
    "#     print(\"Compiled PDF Created\")\n",
    "#     return\n",
    "\n",
    "\n",
    "def get_soup_from_BeautifulSoup(url=\"https://www.google.com/search\", params = None , *headers):\n",
    "    response = requests.get(url, params=params, headers=headers )\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    with open(\"test.html\", mode=\"w+\", encoding='utf-8') as f:\n",
    "        f.write(soup.prettify())\n",
    "        f.close()\n",
    "    return BeautifulSoup(response.text, 'lxml')\n",
    "# using Beautiful Soup\n",
    "# The main content of google search\n",
    "def get_data_list_using_BeautifulSoup(soup):\n",
    "    main_div = soup.find(\"div\", id=\"main\")\n",
    "    # making list of all div tags\n",
    "    one_before_start_div = main_div.findChild(\"div\")\n",
    "    one_before_start_div = one_before_start_div.find_next_siblings(\"div\")\n",
    "    one_before_start_div = one_before_start_div[1]\n",
    "\n",
    "    list_of_divs = one_before_start_div.find_next_siblings(\"div\")\n",
    "    # Removes last elements which has other searches (which div is not imp)\n",
    "    list_of_divs.pop()\n",
    "    # Clearing the memory\n",
    "    # del one_before_start_div, main_div\n",
    "    # Making list of all the data on the google search page\n",
    "    all_info = []\n",
    "    \n",
    "    for the_div in list_of_divs:\n",
    "        link_to_post = the_div.find_next(\"a\").attrs[\"href\"]\n",
    "        if link_to_post[0] == \"/\":\n",
    "            link_to_post = \"https://www.google.com\" + link_to_post\n",
    "\n",
    "        info_list = the_div.find_next(\"div\").find_next(\"div\").find_next_sibling().find_next_sibling()\n",
    "        info_list = info_list.find_next(\"span\").parent.find_all(string=True)\n",
    "        \n",
    "        try:\n",
    "            if info_list[0]==\"People also ask\" or info_list[0]==\"Missing:  \"or info_list[0]== \"Related searches\":\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        title = the_div.find_next(\"h3\").text\n",
    "        # Removes an extra dot in list\n",
    "        info_list.pop(1)\n",
    "        info_list.insert(0, title)\n",
    "        # words_date = info_list[0].split(\" \")\n",
    "        # if int(words_date[0]) > 4 and words_date[1] == \"months\": #If the news is from before 4 months it wont be saved into list\n",
    "        #     continue\n",
    "        # datetime_object = datetime.strptime(info_list[0], '%b %d %Y %I:%M%p')\n",
    "        # print(datetime_object)\n",
    "        # print(link_to_post)\n",
    "        _info = [link_to_post]\n",
    "        # Getting correct(Redirected URL)\n",
    "        _info[0] = requests.get(_info[0],allow_redirects=False)\n",
    "        _info[0]= _info[0].headers['location']\n",
    "        \n",
    "        _info.extend(info_list)\n",
    "        \n",
    "        dict_info = {\n",
    "            \"link_post\":_info[0],\n",
    "            \"title\":_info[1],\n",
    "            \"description\":\"\",\n",
    "            \"publish_date\":\"\"\n",
    "        }\n",
    "        try :\n",
    "            dict_info[\"description\"]=_info[3]\n",
    "            dict_info[\"publish_date\"]=_info[2]\n",
    "            dict_info[\"description\"] = dict_info[\"description\"].replace(u'\\xa0', u' ')\n",
    "        except :\n",
    "            dict_info[\"description\"]=_info[2]\n",
    "            dict_info[\"description\"] = dict_info[\"description\"].replace(u'\\xa0', u' ')\n",
    "        # appending the info into the list\n",
    "        all_info.append(dict_info)\n",
    "        # order in final list is url, published at, description\n",
    "    print(len(all_info))\n",
    "    return all_info\n",
    "# all_info = get_data_list_using_BeautifulSoup(soup)\n",
    "# all_info\n",
    "# print(Extractor(extractor='ArticleExtractor', url=myurl.url).getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "\n",
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n",
      "9 done\n",
      "10 done\n",
      "11 done\n",
      "\n",
      "12 done\n",
      "13 done\n",
      "14 done\n",
      "15 done\n",
      "\n",
      "16 done\n",
      "\n",
      "17 done\n",
      "18 done\n",
      "\n",
      "19 done\n",
      "\n",
      "20 done\n",
      "21 done\n",
      "22 done\n",
      "23 done\n",
      "24 done\n",
      "\n",
      "25 done\n",
      "26 done\n",
      "27 done\n",
      "28 done\n",
      "\n",
      "29 done\n",
      "\n",
      "30 done\n",
      "ScienceDirect.com | Science, health and medical journals, full text articles and books.\n",
      "31 done\n",
      "32 done\n",
      "33 done\n",
      "34 done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"q\": 'sustainability report of Bain and company :pdf',\n",
    "    \"hl\": \"en\",\n",
    "    # \"tbm\": \"nws\",\n",
    "    \"num\": 100, #no. of results per page\n",
    "    #  \"tbs\": \"sbd:1\", #Sort by Date\n",
    "    \"start\": None,\n",
    "}\n",
    "page_number = 1 #change the page number from here\n",
    "params[\"start\"] = params[\"num\"]*(page_number-1)\n",
    "time.sleep(1.2)\n",
    "\n",
    "google_search_results=get_data_list_using_BeautifulSoup(get_soup_from_BeautifulSoup(params))\n",
    "a_set_titles = set()\n",
    "a_set_link = set()\n",
    "browser = webdriver.Chrome()\n",
    "all_links_titles=[]\n",
    "for index,card in enumerate(google_search_results):\n",
    "    for href_dict in get_all_hrefs_of_pdfs(card[\"link_post\"], browser_instance=browser):\n",
    "        all_links_titles.append(href_dict)\n",
    "        a_set_titles.add(href_dict['title'])\n",
    "        a_set_link.add(href_dict['link'])\n",
    "        \n",
    "    print(index, \"done\")\n",
    "browser.close()\n",
    "print(len(a_set_link), len(a_set_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "# open(\"pdf_list.csv\")\n",
    "print(a_set_link)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "daaa31c8034f1feb794b255046b334f8762dacc75c33e103641e2c034660438a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
