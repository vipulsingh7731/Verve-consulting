{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f71ece0a-da63-4ce8-ae1b-c32f943d8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from googlesearch import search\n",
    "query = \"esg sector :pdf\"\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from boilerpipe.extract import Extractor\n",
    "import time\n",
    "from selenium import webdriver\n",
    "import csv\n",
    "\n",
    "# URL='https://www.beautypackaging.com/contents/view_breaking-news/2021-12-13/geka-earns-b-from-cdp-for-reducing-its-environmental-impact/'\n",
    "\n",
    "# extractor = Extractor(extractor='ArticleExtractor', url=URL)\n",
    "# print(extractor.getText())\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "\n",
    "# Beautiful Soup implementation\n",
    "def get_soup_from_BeautifulSoup(params, *headers):\n",
    "    response = requests.get(\"https://www.google.com/search\", params=params, headers=headers )\n",
    "    \n",
    "    return BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# Selenium Implementation\n",
    "def get_html_source(url, sleep_time=10):\n",
    "    browser = webdriver.Chrome()\n",
    "    # get web page\n",
    "    browser.get(url)\n",
    "    # execute script to scroll down the page\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "    # sleep for time\n",
    "    time.sleep(sleep_time)\n",
    "    return browser.page_source\n",
    "\n",
    "def get_google_url(params):\n",
    "    params_key, params_value = list(params.keys()),list(params.values())\n",
    "    url = f\"https://www.google.com/search?\"\n",
    "    for i in range(len(params)):\n",
    "        url = url + params_key[i] + \"=\" + str(params_value[i]).replace(\" \", \"+\")+ \"&\"\n",
    "    return url\n",
    "\n",
    "def get_soup_from_Selenium(params, *sleep_time):\n",
    "    if not sleep_time:\n",
    "        sleep_time = (10,)\n",
    "    params_key, params_value = list(params.keys()),list(params.values())\n",
    "    url = f\"https://www.google.com/search?\"\n",
    "    for i in range(len(params)):\n",
    "        url = url + params_key[i] + \"=\" + str(params_value[i]).replace(\" \", \"+\")+ \"&\"\n",
    "    return BeautifulSoup(get_html_source(url, sleep_time[0]), 'html.parser')\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Items in This Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cfe56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" ,\n",
    "    'referer':'https://www.google.com/'\n",
    "}\n",
    "# Google Search Parameters LIST https://stenevang.wordpress.com/2013/02/22/google-advanced-power-search-url-request-parameters/\n",
    "params = {\n",
    "    \"q\": 'new climate protocols \"$\"',\n",
    "    \"hl\": \"en\",\n",
    "    \"tbm\": \"nws\",\n",
    "    \"num\": 100, #no. of results per page\n",
    "    #  \"tbs\": \"sbd:1\", #Sort by Date\n",
    "    \"start\": None,\n",
    "}\n",
    "page_number = 1 #change the page number from here\n",
    "params[\"start\"] = params[\"num\"]*(page_number-1)\n",
    "time.sleep(1.2)\n",
    "\n",
    "soup = get_soup_from_Selenium(params, 5)\n",
    "# soup = get_soup_from_BeautifulSoup(params)\n",
    "\n",
    "with open(\"test.html\", mode=\"w+\") as f:\n",
    "    f.write(soup.prettify())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab0f031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail for https://www.seattletimes.com/sports/kraken/kraken-blue-jackets-notes-from-geoff-baker-for-sunday-print/\n",
      "HTTPSConnectionPool(host='www.seattletimes.com', port=443): Read timed out. (read timeout=30)\n"
     ]
    }
   ],
   "source": [
    "# Using Chrome Web Driver\n",
    "\n",
    "def get_data_list_using_chrome_driver(soup):\n",
    "    \n",
    "    g_cards = soup.find_all(\"g-card\", id=\"\")\n",
    "    all_info = []\n",
    "    # iteration = 0\n",
    "    for a_card in g_cards:\n",
    "        link_post = a_card.find(\"a\")[\"href\"]\n",
    "        data_list = list(a_card.strings)\n",
    "        title = data_list[1]\n",
    "        title = title.replace(\"...\", \".\")\n",
    "        title = title.replace(\"\\n\", \"\")\n",
    "        description = data_list[2]\n",
    "        description = description.replace(\"\\n\", \"\")\n",
    "        description = description.replace(\"...\", \".\")\n",
    "        publish_date = data_list[4]\n",
    "        _info = {\n",
    "            \"link_post\":link_post,\n",
    "            \"title\":title,\n",
    "            \"description\":description,\n",
    "            \"publish_date\":publish_date,\n",
    "            \"article\": \"\"\n",
    "            }\n",
    "        try:\n",
    "            response = requests.get(_info[\"link_post\"], timeout=30)\n",
    "            _info[\"article\"] = Extractor(extractor='ArticleExtractor', html=response.text).getText()\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"fail for\", _info[\"link_post\"])\n",
    "            _info[\"article\"] = \"Error\"\n",
    "            print(e)\n",
    "        all_info.append(_info)\n",
    "        # iteration +=1\n",
    "        # print(iteration)\n",
    "    return all_info\n",
    "the_data = get_data_list_using_chrome_driver(soup)\n",
    "the_data\n",
    "\n",
    "def csv_write_data(list_of_data, file_name, write_mode):\n",
    "    with open(file_name, write_mode, encoding=\"utf-8\") as file:\n",
    "        csv_writer = csv.DictWriter(file, fieldnames=list_of_data[0].keys())\n",
    "        csv_writer.writeheader()\n",
    "        for line in list_of_data:\n",
    "            try:\n",
    "                csv_writer.writerow(line)\n",
    "            except Exception as error:\n",
    "                print(\"Error in writing to CSV\", error)\n",
    "        file.close()\n",
    "    return\n",
    "\n",
    "csv_write_data(the_data,\"mydata.csv\",\"w\")\n",
    "\n",
    "file =  open(\"mydata.csv\", \"r\",encoding =\"utf-8\")\n",
    "csv_reader = list(csv.DictReader(file))\n",
    "file.close()\n",
    "# list(csv_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc8ee146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retailer\n",
      "ACE’s Low Carbon Strategy Pays Dividends\n",
      "A new USDA program could soon establish the quantification and verification protocols necessary for long-awaited climate-smart farming incentives, which could begin to scale ethanol’s role in an increasingly  decarbonized transportation sector.\n",
      "By Brian Jennings       | December 20, 2021\n",
      "ADVERTISEMENT\n",
      "Given persistent challenges regarding the Environmental Protection Agency’s mismanagement of the RFS, ACE has been taking direction from our Board of Directors to be proactive about opportunities to expand ethanol demand through new clean fuel or low carbon policies and markets.\n",
      "According to the United States Department of Agriculture, U.S. farmers currently store over 20 million metric tons of carbon per year and they can store an additional 180 million metric tons per year representing 12-14 percent of U.S. carbon emissions annually. The Department of Energy’s Argonne National Laboratory has found that deploying specific crop rotation systems in the upper Great Plains would result in increased carbon sequestration, reducing the carbon intensity of agricultural production, and generating hundreds of dollars per acre in revenue if credited in state low carbon fuel markets.\n",
      "Despite this promise, established mandatory carbon markets like the California Low Carbon Fuel Standard do not yet credit greenhouse gas (GHG) reductions achieved through carbon sequestering conservation practices like no-till, cover crops and nitrogen management undertaken in ethanol feedstock production. These regulators want better localized quantification, verification and modeling protocols in order to grant access to these low carbon markets. These same themes permeate discussions in Washington D.C. on the theory of using low carbon fuel programs to drive decarbonization goals, and in the Midwestern states when discussing the specifics that should be included in new clean fuel policies and markets.\n",
      "That is why ACE partnered with South Dakota Corn, Dakota Ethanol, South Dakota State University and Cultivating Conservation to leverage $7.5 million in USDA funding to document ethanol’s net-negative carbon potential in the real world. Under this ACE-USDA program, farmers will be compensated for adopting climate-smart agricultural practices that sequester carbon, reduce GHG emissions, and improve soil health. Importantly, the partnership will quantify the resulting soil health and GHG benefits, correlate them with existing models, and develop a non-proprietary verification system to secure the first approved access to LCFS markets based on these on-farm conservation practices.\n",
      "The project is leveraging USDA funds to open low carbon fuel markets for farmers and ethanol companies. In the South Dakota project area, this access could mean upwards of $10 million in additional revenue annually because of the GHG contributions from on-farm conservation practices.\n",
      "The new ACE-USDA project answers the question of “how” for one ethanol facility corn draw. Beyond that, it also illuminates the path forward for the industry.\n",
      "Recently, USDA Secretary Tom Vilsack announced a new Climate Smart Agriculture and Forestry Partnership Program designed to reduce barriers to climate markets such as LCFS programs and leverage USDA funds to develop new revenue streams for farmers. It is a significant opportunity to utilize USDA resources to gain broader access to existing state low carbon fuel markets. It is an even more significant opportunity to establish the quantification and verification protocols necessary to scale ethanol’s ability to power a sizable piece of a decarbonized transportation sector into the future.\n",
      "I realize some people are concerned all this talk about climate change may spell trouble for liquid fuels because too many politicians lazily think electric vehicles are the only transportation-related climate solution, but ACE is confident ethanol is the best low carbon option to replace petroleum and provide meaningful GHG reductions.\n",
      "That is why we have been highlighting climate-smart farming practices though this new USDA project, and we will keep taking proactive steps to ensure corn ethanol is part of the climate solution.\n",
      "Our hard work is already yielding important results for farmers and ethanol producers who want to seize opportunities to increase demand based on ethanol’s low carbon value in new clean fuel markets.\n",
      "Author: Brian Jennings\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcff21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Beautiful Soup\n",
    "# The main content of google search\n",
    "def get_data_list_using_BeautifulSoup(soup):\n",
    "    main_div = soup.find(\"div\", id=\"main\")\n",
    "    # making list of all div tags\n",
    "    one_before_start_div = main_div.findChild(\"div\")\n",
    "    one_before_start_div = one_before_start_div.find_next_siblings(\"div\")\n",
    "    one_before_start_div = one_before_start_div[1]\n",
    "\n",
    "    list_of_divs = one_before_start_div.find_next_siblings(\"div\")\n",
    "\n",
    "    # Clearing the memory\n",
    "    # del one_before_start_div, main_div\n",
    "    # Making list of all the data on the google search page\n",
    "    all_info = []\n",
    "    for the_div in list_of_divs:\n",
    "        link_to_post = the_div.find_next(\"a\").attrs[\"href\"]\n",
    "        if link_to_post[0] == \"/\":\n",
    "            link_to_post = \"https://www.google.com\" + link_to_post\n",
    "\n",
    "        info_list = the_div.find_next(\"div\").find_next(\"div\").find_next_sibling().find_next_sibling()\n",
    "        info_list = info_list.find_next(\"span\").parent.find_all(string=True)\n",
    "        \n",
    "        title = the_div.find_next(\"h3\").text\n",
    "        # Removes an extra dot in list\n",
    "        info_list.pop(1)\n",
    "        info_list.insert(0, title)\n",
    "        # words_date = info_list[0].split(\" \")\n",
    "        # if int(words_date[0]) > 4 and words_date[1] == \"months\": #If the news is from before 4 months it wont be saved into list\n",
    "        #     continue\n",
    "        # datetime_object = datetime.strptime(info_list[0], '%b %d %Y %I:%M%p')\n",
    "        # print(datetime_object)\n",
    "        # print(link_to_post)\n",
    "        # print(info_list)\n",
    "        _info = [link_to_post]\n",
    "        # OPTIONAL updating the redirected URL to full URL, this will impact performance!!\n",
    "        # _info[0] = requests.get(_info[0],headers=headers).url\n",
    "        _info.extend(info_list)\n",
    "        dict_info = {\n",
    "            \"link_post\":_info[0],\n",
    "            \"title\":_info[1],\n",
    "            \"description\":_info[2],\n",
    "            \"publish_date\":_info[3]\n",
    "        }\n",
    "        # appending the info into the list\n",
    "        all_info.append(dict_info)\n",
    "        # order in final list is url, published at, description\n",
    "    return all_info\n",
    "# all_info = get_data_list_using_BeautifulSoup(soup)\n",
    "# all_info\n",
    "# print(Extractor(extractor='ArticleExtractor', url=myurl.url).getText())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
