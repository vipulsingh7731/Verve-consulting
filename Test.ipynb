{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f71ece0a-da63-4ce8-ae1b-c32f943d8e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\\n<html>\\n<head><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"><meta name=\"viewport\" content=\"initial-scale=1\"><title>https://www.google.com/search?q=+new+climate+protocols+&amp;hl=en&amp;num=10&amp;start=20</title></head>\\n<body style=\"font-family: arial, sans-serif; background-color: #fff; color: #000; padding:20px; font-size:18px;\" onload=\"e=document.getElementById(\\'captcha\\');if(e){e.focus();}\">\\n<div style=\"max-width:400px;\">\\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\"><br>\\n<form id=\"captcha-form\" action=\"index\" method=\"post\">\\n<script src=\"https://www.google.com/recaptcha/api.js\" async defer></script>\\n<script>var submitCallback = function(response) {document.getElementById(\\'captcha-form\\').submit();};</script>\\n<div id=\"recaptcha\" class=\"g-recaptcha\" data-sitekey=\"6LfwuyUTAAAAAOAmoS0fdqijC2PbbdH4kjq62Y1b\" data-callback=\"submitCallback\" data-s=\"45ldaFytfZWYDwaEGWjGLAvmCn56sMC6CZQ234UH8FRu_9KO9CSr8JB80FpvBHx5DH683g4m8fpxoWgJ6YIf-IYwcjUXo1cppCEeKjh5aXhFvdnmVWeQEZO4MpDiIQwHGp-CPtUfEIXWZZ8C9S9C5HFmm8p57vc9oUr5nwg17x5PVY2N91N26azoGYcMRgsJAiHDTUresBSXeKM509gnSR2bHmTPWtXXpZTr3rM\"></div>\\n<input type=\\'hidden\\' name=\\'q\\' value=\\'EgS0vPN3GKLR5o0GIhCzfJWg8LbFu55lKJiPAsEtMgFy\\'><input type=\"hidden\" name=\"continue\" value=\"https://www.google.com/search?q=+new+climate+protocols+&amp;hl=en&amp;num=10&amp;start=20\">\\n</form>\\n<hr noshade size=\"1\" style=\"color:#ccc; background-color:#ccc;\">\\n\\n<div style=\"font-size:13px;\">\\n<b>About this page</b><br><br>\\n\\nOur systems have detected unusual traffic from your computer network.  This page checks to see if it&#39;s really you sending the requests, and not a robot.  <a href=\"#\" onclick=\"document.getElementById(\\'infoDiv\\').style.display=\\'block\\';\">Why did this happen?</a><br><br>\\n\\n<div id=\"infoDiv\" style=\"display:none; background-color:#eee; padding:10px; margin:0 0 15px 0; line-height:1.4em;\">\\nThis page appears when Google automatically detects requests coming from your computer network which appear to be in violation of the <a href=\"//www.google.com/policies/terms/\">Terms of Service</a>. The block will expire shortly after those requests stop.  In the meantime, solving the above CAPTCHA will let you continue to use our services.<br><br>This traffic may have been sent by malicious software, a browser plug-in, or a script that sends automated requests.  If you share your network connection, ask your administrator for help &mdash; a different computer using the same IP address may be responsible.  <a href=\"//support.google.com/websearch/answer/86640\">Learn more</a><br><br>Sometimes you may be asked to solve the CAPTCHA if you are using advanced terms that robots are known to use, or sending requests very quickly.\\n</div>\\n\\nIP address: 180.188.243.119<br>Time: 2021-12-15T08:34:43Z<br>URL: https://www.google.com/search?q=+new+climate+protocols+&amp;hl=en&amp;num=10&amp;start=20<br>\\n</div>\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2.pdf import setRectangle\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import io\n",
    "import PyPDF2\n",
    "from googlesearch import search\n",
    "query = \"esg sector :pdf\"\n",
    "import requests, lxml\n",
    "from bs4 import BeautifulSoup\n",
    "from boilerpipe.extract import Extractor\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# URL='https://www.beautypackaging.com/contents/view_breaking-news/2021-12-13/geka-earns-b-from-cdp-for-reducing-its-environmental-impact/'\n",
    "\n",
    "# extractor = Extractor(extractor='ArticleExtractor', url=URL)\n",
    "\n",
    "# print(extractor.getText())\n",
    "# headers = {\n",
    "#     \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19582\"\n",
    "header = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\" ,\n",
    "    'referer':'https://www.google.com/'\n",
    "}\n",
    "# PARAMETERS LIST https://stenevang.wordpress.com/2013/02/22/google-advanced-power-search-url-request-parameters/\n",
    "params = {\n",
    "    \"q\": \"\"\" new climate protocols \"\"\",\n",
    "    \"hl\": \"en\",\n",
    "    # \"tbm\": \"nws\",\n",
    "    \"num\": 10,\n",
    "    #  \"tbs\": \"sbd:1\", #Sort by Date\n",
    "    \"start\": None,\n",
    "}\n",
    "page_number = 3 #change the page number from here\n",
    "params[\"start\"] = params[\"num\"]*(page_number-1)\n",
    "time.sleep(1.2)\n",
    "response = requests.get(\"https://www.google.com/search\", params=params)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "with open(\"test.html\", mode=\"w+\") as f:\n",
    "    # pdf = PyPDF2.PdfFileReader(io.BytesIO(html_page.content))\n",
    "    f.write(soup.prettify())\n",
    "    # f.write(pdf.getPage(0))\n",
    "f.close()\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1bcff21a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findChild'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VIPULK~1\\AppData\\Local\\Temp/ipykernel_18300/1674181413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmain_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"main\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# making list of all div tags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mone_before_start_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindChild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mone_before_start_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_before_start_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_next_siblings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mone_before_start_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_before_start_div\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findChild'"
     ]
    }
   ],
   "source": [
    "# The main content of google search\n",
    "main_div = soup.find(\"div\", id=\"main\")\n",
    "# making list of all div tags\n",
    "one_before_start_div = main_div.findChild(\"div\")\n",
    "one_before_start_div = one_before_start_div.find_next_siblings(\"div\")\n",
    "one_before_start_div = one_before_start_div[1]\n",
    "\n",
    "list_of_divs = one_before_start_div.find_next_siblings(\"div\")\n",
    "\n",
    "# Clearing the memory\n",
    "del one_before_start_div, main_div\n",
    "# Making list of all the data on the google search page\n",
    "all_info = []\n",
    "for the_div in list_of_divs:\n",
    "    link_to_post = the_div.find_next(\"a\").attrs[\"href\"]\n",
    "    if link_to_post[0] == \"/\":\n",
    "        link_to_post = \"https://www.google.com\" + link_to_post\n",
    "\n",
    "    info_list = the_div.find_next(\"div\").find_next(\"div\").find_next_sibling().find_next_sibling()\n",
    "    info_list = info_list.find_next(\"span\").parent.find_all(string=True)\n",
    "    \n",
    "    info_list.pop(1)\n",
    "    # words_date = info_list[0].split(\" \")\n",
    "    # if int(words_date[0]) > 4 and words_date[1] == \"months\": #If the news is from before 4 months it wont be saved into list\n",
    "    #     continue\n",
    "    # datetime_object = datetime.strptime(info_list[0], '%b %d %Y %I:%M%p')\n",
    "    # print(datetime_object)\n",
    "    # print(link_to_post)\n",
    "    # print(info_list)\n",
    "    _info = [link_to_post]\n",
    "    _info.extend(info_list)\n",
    "    # OPTIONAL updating the redirected URL to full URL, this is impact performance!!\n",
    "    # _info[0] = requests.get(_info[0],headers=headers).url\n",
    "    # appending the info into the list\n",
    "    all_info.append(_info)\n",
    "print(len(all_info))\n",
    "all_info\n",
    "# print(Extractor(extractor='ArticleExtractor', url=myurl.url).getText())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
